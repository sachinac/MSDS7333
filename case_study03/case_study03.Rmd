---
title: 'MSDS 7333 Spring 2021: Case Study 03 '
author: "Sachin Chavan,Tazeb Abera,Gautam Kapila,Sandesh Ojha"
date: "`r format(Sys.time(), '%Y %B %d')`"
output:
  pdf_document:
    keep_tex: yes
    extra_dependencies: float
  word_document: default
  html_document:
    df_print: paged
subtitle: Email Spam Detection
header-includes:
- \usepackage{siunitx}
- \newcolumntype{d}{S[table-format=3.2]}
- \usepackage{multicol}  
- \usepackage{float}  
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[L]{Case Study 03}
latex_engine: pdflatex
urlcolor: gray    
---


   
```{r,echo=FALSE,message=FALSE,warning=FALSE}
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
library(kableExtra)
library(ggplot2)
library(purrr)
library(tidyr)
library(dplyr)
library(rpart)
library(rattle)
library(pander)
library(ggcorrplot)
library(caret)
library(ROSE)
library(naniar)
library(rpart.plot)
library(mlr)
source('src/cs03_methods.R')
theme_set(theme_bw())
```

```{r, echo=FALSE,cache=TRUE}
#lossmatrix <- matrix(c(0,1,2,0), byrow = TRUE, nrow = 2)
#mytree <- rpart(
#  isSpam ~ . , 
#  data = emailDFrp, 
#  method = "anova",
#  maxdepth = 5, 
#  minsplit = 2, 
#  minbucket = 1,
#  parms = list(loss = lossmatrix)
#)
#printcp(mytree)
#mytree <- prune(mytree, cp = 0.08)
#fancyRpartPlot(mytree, caption = NULL)
```

```{r, echo=FALSE,cache=TRUE}
load('data/data.Rda')
```

# Introduction

**E**mail spams are unsolicited emails that also referred as Junk email sent in bulk. There is no law at least in United States that prevents anybody from sending unsolicited emails whether in single email or in bulk. Research indicate that email spams are accounted for over 80% of total email traffic.$^{[2][3][4]}$. There are different forms of email spams or unsolicited emails that may affect either individuals or organizations. Following are the few of them :

* Commercial Advertisements
* Hoax emails
* Emails Spoofs or phishing
* Lottery winining notification is very common
* Money Scams
* Virus/Malware

All these can adversely affect individual as well as organizations. Such emails comes from unknown sources that we never interacted with or sometimes spammers use spoofing techniques to mislead recipients to appear as valid source. Organizations also takes advantage to send bulk emails to potential customers to advertise their products.Such bulk emails occupies network traffic all the time. Human can easily categorize suche mails and trash them manaually 

Now a days different machine learning algorithm are used for detecting and daily inspecting for incoming emails by different email provider companies like google, yahoo and the like. Among these machine learning algorithms decision tree methods are one of the best list.  

\newpage
# Business Understanding

The two common approaches used for filtering spam mails are knowledge engineering and machine learning. Emails are classified as either spam or ham using a set of rules in knowledge engineering $^{[4]}$. Machine learning approach have proved to be more efficient than knowledge engineering approach. No rule is required to be specified, rather a set of training samples which are pre-classified email messages are provided. A particular machine learning algorithm is then used to learn the classification rules from these email messages. 

Machine learning algorithms use statistical models to classify data. In the case of spam detection, a trained machine learning model must be able to determine whether the sequence of words found in an email are closer to those found in spam emails or safe ones.

Several studies have been carried out on machine learning techniques and many of these algorithms are being applied in the field of email spam filtering. Examples of such algorithms include Deep Learning, NaÃ¯ve Bayes, Decision tree, Support Vector Machines, Neural Networks, K-Nearest Neighbor, Rough sets, and Random Forests.

For this case study we are implementing a a decision tree algorithms. Decision tree learning is the predictive modeling approaches used in statistics, data mining and machine learning. It uses a decision tree to go from observations about an item to conclusions about the item's target value. [Wikipedia]. We explore a decision tree package in R called rpart, which is short for recursive partitioning. 

Our objective is to investigate and optimize key hyperparameters used in the rpart package in order to classify email messages as spam or Ham email. In order to accomplish this, we fit a default decision tree and an optimized decision tree and compare them. 

\newpage

# Data Evaluation / Engineering

The dataset provided for this cases study was preprocessed and labeled already and contains features extracted from after preprocessing. It contains total of 9348 unique emails with 29 predictor variables and one response variable named isSpam. Of the 30 total variables, 17 are boolean factor variables and the remaining 13 variables are numeric variables. Each email has been previously classified as spam or valid. This dataset will be used to build a model using decision trees to classify email messages as spam or ham. Details of each feature is as follows:

## Factor variables

```{r, comment=NA,echo=FALSE,cache=TRUE}
factor_cols <- emailDFrp %>% keep(is.factor)  %>% colnames()
factor_desc <- c("Target Variable (T=Spam/F=Ham)",
                 "T=If subject starts with Re: F=Otherwise","T=If FROM email address field contains underscore F=Otherwise",
                 "T=If priority key present in the header F=Otherwise","T=If inReplyTo present in the header F=Otherwise",
                 "T=If Recipient's email addresses are sorted F=Otherwise","T=if Subject line contains punctuation F=Otherwise",
                 "T=If MIME type is multipart/text F=otherwise","T=If email body contains images F=Otherwise",
                 "T=If message contains PGP signature F=Otherwise","T=If subject contains one of the words from spam vector F=Otherwise",
                 "T=If there is no hostname in header F=Otherwise","T=If sender's email address ends with number",
                 "T=If subject line contains all uppercase letters F=Otherwise","T=If Message body contains word original message F=Otherwise",
                 "T=If word dear found in message body F=Otherwise","T=if message body contains word Wrote: F=Otherwise"
                 )

text_tbl <- data.frame(
Feature = factor_cols,
Description = factor_desc
)
kbl(text_tbl, booktabs = T) %>%
kable_styling(full_width = F) %>% column_spec(2, width = "30em")
```

## Numeric variables

```{r, comment=NA,echo=FALSE,cache=TRUE}
numeric_cols <- emailDFrp %>% keep(is.numeric)  %>% colnames()
numeric_desc <- c("Number of Lines in the message","Number of characters in the messsage ",
                 "Number of Exclamation in Subject line","Number of question marks in subject line",
                 "Number of email attachments","Number of Recipients in the email message",
                 "Percentage of uppercase letters in the Body of the message","Hour of the day",
                 "Percentage of HTML tags in the body of the message","Percentage blanks in Subject line",
                 "Number of consecutive forward symbols in the body of the message","Average length of the word in the body of the message",
                 "Number of dollar symbol in the message body"
                 )
text_tbl <- data.frame(
Feature = numeric_cols,
Description = numeric_desc
)
kbl(text_tbl, booktabs = T) %>%
kable_styling(full_width = F) %>% column_spec(2, width = "30em")
```

## Structure of dataframe

As shown below this dataframe contains 30 columns and 9348 observations.

```{r,comment=NA, echo=FALSE,cache=TRUE}
str(emailDFrp,vec.len=3)
```

## Summary of Factor Variables
```{r,comment=NA, echo=FALSE,cache=TRUE}
emailDFrp %>% keep(is.factor) %>% summary()
```

## Summary of Numeric Variables

```{r,comment=NA, echo=FALSE,cache=TRUE}
emailDFrp %>% keep(is.numeric) %>% summary()
```

Summary shows missing values in subSpamWords(7),noHost(1),isYelling(7) from factor variables and in subExcCt(20), subQuesCt(20), numRec(282),subBlanks(20) from numeric fields

## Missing Values : Visualization

```{r, comment=NA,include=TRUE, cache=TRUE, echo=FALSE, warning=FALSE,message=FALSE, fig.cap=cap,fig.width=6,fig.height=5,fig.align='center',fig.fullwidth=FALSE}
gg_miss_var(emailDFrp,show_pct = FALSE,facet=isSpam)+
  labs(y = "All Missing at once ")
cap <- "Left : Not Spam, Right : Spam"
```

Interpretation: 

* noHost is missing on single record when all other values are present
* subSpamWords & isYelling are missing on same 7 records when all other values are present
* subExcCt, subQuesCt, subBlanks appears to be missing on same 20 records
* It is observed that 13 of 20 rows in subBlanks are NaN.
* numRec has missing values on 282 records most likely due to recipients were added in BCC and only 12 emails have been flagged as Spam and remaining 270 are not spam. These 282 missing values are appears to be random and satisfies MAR (Missing at Random). Missing values will be fixed using mean imputation for both categories (Spam and not Spam) seperately.

## Missing Values : Assumptions and Actions

Instead of deleting records with missing values following actions will be taken on the missing data. Imputation will be applied to numRec variable.

```{r, comment=NA,echo=FALSE,cache=TRUE}
FeatureName <- c("noHost","subSpamWords","isYelling", "subExcCt", "subQuesCt", "numRec","subBlanks")
DataType    <- c("factor","factor","factor","numeric","numeric","numeric","numeric")
Missing     <- c(1,7,7,20,20,282,20)
Assumption  <- c("HostName missing","Subject line Missing","Subject Line Missing",
                "Subject Line without exclamation mark","Subject Line without question mark","Undisclosed recipients","Subject Line Witout Blanks")
Action      <- c("Replace NAs with F","Replace NAs with F","Replace NAs with F",
                 "Replace NAs with 0s","Replace NAs with 0s","Apply mean imputation to Spam and Not Spam groups seperately","Replace NAs with 0s")

text_tbl <- data.frame(
FeatureName = FeatureName,
DataType = DataType,
Missing = Missing,
Assumption = Assumption,
Action = Action
)

kbl(text_tbl, booktabs = T) %>% kable_styling(full_width = F) %>% column_spec(5, width = "15em")
email_df <- emailDFrp 

email_df[c(which(is.na(email_df$subSpamWords))),]$subSpamWords <- as.factor("F")
email_df[c(which(is.na(email_df$noHost))),]$noHost             <- as.factor("F")
email_df[c(which(is.na(email_df$isYelling))),]$isYelling       <- as.factor("F")

email_df[c(which(is.na(email_df$subExcCt))),]$subExcCt   <- 0
email_df[c(which(is.na(email_df$subQuesCt))),]$subQuesCt <- 0
email_df[c(which(is.na(email_df$numRec))),]$numRec       <- 0
email_df[c(which(is.na(email_df$subBlanks))),]$subBlanks <- 0
```

## No Missing data in Final DataFrame

```{r, comment=NA,echo=FALSE,cache=TRUE, fig.width=6,fig.height=4, fig.align='center',fig.cap=cap}
gg_miss_var(email_df,show_pct = FALSE,facet=isSpam)+
  labs(y = "All Missing at once ")
cap <- "Left : Not Spam, Right : Spam"
```

### Class Distribution (Target Variable)

Below chart shows target variable is binary and class lables are not evenly distributed. This is the case of imbalanced dataset.

```{r, comment=NA,echo=FALSE,cache=TRUE, fig.width=2,fig.height=2, fig.align='center',fig.cap="Target Variable"}
bar <- modified_df(email_df)

g <- ggplot(bar,aes(x=isSpam,y=Total))
g +  geom_bar(stat="identity", width = 0.4, fill="tomato2") + 
     labs(title="Spam vs No Spam" ) +
     xlab("Email Spams")+
     theme(plot.title = element_text(hjust = 1.2))+
     ggthemes::theme_excel_new()
```


```{r, include=TRUE, echo=FALSE, cache=TRUE,out.align='left',fig.pos="top",results='markup',fig.cap="Target Percentage Distribution"}
#pandoc.table(bar,stype="rmakrdown",justify="left")
kbl(bar, caption="Target Percentage Distribution",booktabs = T) %>% kable_styling(full_width = F,latex_options = c("hold_position")) 
```

### Correlation Plot

This plot shows numLines and bodyCharCt are strongly correlated at 90% and one of them can be removed. So bodyCharCt will be removed from further analysis.

```{r, comment=NA,echo=FALSE,cache=TRUE, fig.width=5,fig.height=4, fig.align='center',fig.cap="Correlogram"}
corr <- round(cor(email_df[,c(18:30)]), 1)
#ggcorrplot(corr, hc.order = TRUE, 
#           type = "lower", 
#           lab = TRUE, 
#           lab_size = 2, 
#           method="circle", 
#           colors = c("tomato2", "white", "springgreen3"), 
#           title="Correlogram of Numeric Features", 
#           ggtheme=theme_bw)

ggcorrplot(corr, hc.order = TRUE, type = "lower",
   outline.col = "white",
   ggtheme = ggplot2::theme_gray,
   colors = c("#6D9EC1", "gray", "#E46726"))

email_final <- email_df[,c(1:18,20:30)]
```

## Strcuture of Final DataFrame
```{r, include=TRUE, echo=FALSE, cache=TRUE,out.align='left',fig.pos="top",results='markup'}
str(email_final)
```

## Final Dataframe for analysis 

Structure of DataFrame has not changed. In previous steps only missing values have been replaced with 0s or Fs based on assumption made. Here is sample DataFrame.

```{r, include=TRUE, echo=FALSE, cache=TRUE,out.align='left',fig.pos="top",results='markup'}
knitr::kable(email_final[1:7,c(1:3,18:24)], caption="DataFrame", row.names = FALSE,format = "latex",position = "!b")
```

## Train/Test split

```{r, comment=NA,echo=FALSE,cache=TRUE}
# Split the data into training and test set
set.seed(123)
training.samples <- email_final$isSpam %>%  
                    createDataPartition(p = 0.8, list = FALSE)
train.data <- email_final[training.samples, ]
test.data  <- email_final[-training.samples, ]
```

Original data to be split into train(0.8) and test set (0.2). R package caret provides library function createDataPartition to split data into train/test set and it performs stratified sampling by default as per its documentation. Stratified sampling technique preserves percentage of samples of each class. Target variable in this case is binary i.e. email could either be Spam or not Spam and only approximately 25% emails have been labeled as Spam therefore Stratified sampling is the best way to proceed.

### Train - Target Percentage Distribution


**createDataPartition** function from **caret** packages was used to split data into train/test set.Target variable is binary and distribution of classes is not balanced. i.e. 70% observations are not spam and only 30% are labeled as Spam in this dataset. This is case of imbalanced dataset. Random Stratified sampling has been used to split data into train/test set for building model using decision trees. **Stratified Sampling** preserves the percentage of samples of each class. Scaling and normalization is required for decision tree models.

```{r, include=TRUE, echo=FALSE, cache=TRUE,out.align='left',fig.pos="top",results='markup'}
bar1 <- modified_df(train.data)
kbl(bar1, caption="Target Percentage Distribution",booktabs = T) %>% kable_styling(full_width = F,latex_options = c("hold_position") )
```

## Constraints

 Following are limitations of Decision tree algorithms: 
 
* Decision Tree algorithms are greedy algorithms, prone to overfitting.If not handled properly tree tends to grow larger and become complex and difficult to interpret. Overfitting affects generalizability and hence accuracy of prediction goes down significantly.
* Trees are unstable, meaning that even small change in information leads to big change in trees.
* Loses lot of information in splitting process.
* Bad for large datasets

However, there are different ways to tackle these problems. Hyperparameter tunining, building stopping criteria are discussed in next sections.

\newpage

# Modeling Preparations

The primary objective of this case study is to detect Spam emails. This is binary classification problem. There are bunch of algorithms in the machine learning toolkit to solve classification problems. Logistic regression, Naive Bayes, kNN, SVM and Decision Trees

**Hyperparameters**

Here are decision tree hyperparameters

```{r, comment=NA,include=TRUE, echo=FALSE, messages=FALSE,cache=TRUE,out.align='left',fig.pos="top",results='markup'}
treeTask <- makeClassifTask(data = train.data, target = "isSpam")
tree <- makeLearner("classif.rpart")
getParamSet(tree)
```

\newpage
# Model Building & Evaluation

Decision tree algorithm are prone to overfit quickly and loses generalizability for prediction i.e. prediction performance become poor. Solution to this problem is hyperparameter tuning. Decision trees have several hyparameters that can be tuned to get optimal model. Below table lists important hyperparameters of which minsplit,minbucket, cp and maxdepth will be used here to tune the model. These are the stopping criterion for the trees which can be applied at each stage during tree-building process.


```{r, include=TRUE, echo=FALSE, cache=TRUE,out.align='left',fig.pos="top",results='markup'}
hyper_parameter <- c("minsplit","minbucket","cp","maxdepth", "maxsurrogate","usesurrogate","surrogatestyle")
hp_desc      <-    c("Minimum number of observations required to split the given node. If it is set to 20 indicates that if data has less than 20 records then root node becomes leaf
                   node.","Minimum number of observations required at leaf node. default is minbucket = minsplit/3",
                   "Complexity Parameter to control size of the tree and also known as cost of adding another variable to decision tree",
                   "Set the maximum depth of any node of the final tree",
                   "Used if data contains missing values. useful to determine which split can be used for missing data",
                   "Controls use of surrogate split",
                   "Controls selection of best surrogate"
                 )
text_tbl <- data.frame(
HyperParameters = hyper_parameter,
Description = hp_desc
)
kbl(text_tbl, booktabs = T) %>%
kable_styling(full_width = F) %>% column_spec(2, width = "30em")
```

*getParamSet(tree)* function from **R package mlr** returns following list of hyper-parameters for the decision tree.  

```{r, comment=NA,include=TRUE, echo=FALSE, cache=TRUE,out.align='left',fig.pos="top",results='markup'}
# Creating the task and learner
treeTask <- makeClassifTask(data = train.data, target = "isSpam")
tree <- makeLearner("classif.rpart")
getParamSet(tree)
```

**Tuned Parameters**

Parameters are tuned with following criteria
  
*  5 <= minsplit  <=20
*  3 <= minbucket <=10
*  3 <= maxdepth  <=10
*  0.01 <= cp <=0.1
*  10 fold cross validation
*  Random search Max iterations 200

Here are tuning results:
 
```{r, comment=NA,include=TRUE, message=FALSE, echo=FALSE, warning=FALSE,cache=TRUE,out.align='left',fig.pos="top",results='markup'}

# Defining the hyperparameter space for tuning
tunedTree <- "data/TunedTreeParms.dat"
if (file.exists(tunedTree)) {
  load(file=tunedTree)
} else {
treeParamSpace <- makeParamSet(
  makeIntegerParam("minsplit", lower = 5, upper = 20),
  makeIntegerParam("minbucket", lower = 3, upper = 10),
  makeNumericParam("cp", lower = 0.01, upper = 0.1),
  makeIntegerParam("maxdepth", lower = 3, upper = 10))

# Defining the random search
randSearch  <- makeTuneControlRandom(maxit = 200)
cvForTuning <- makeResampleDesc("CV", iters = 10)

tunedTreePars <- tuneParams(tree, task = treeTask,
                            measures = list(f1,tpr,ppv,tnr),
                            resampling = cvForTuning,
                            par.set = treeParamSpace,
                            control = randSearch)
save(tunedTreePars, file=tunedTree)
}
tunedTreePars
```


```{r, comment=NA,include=TRUE, message=FALSE, echo=FALSE, warning=FALSE,cache=TRUE,out.align='left',fig.pos="top",results='markup'}
# Cross-validating the model-building process
cvTuning <- "data/cvWithTuning.dat"
if (file.exists(cvTuning)) {
  load(file=cvTuning)
} else {
  outer <- makeResampleDesc("CV", iters = 10)
  
  treeParamSpace <- makeParamSet(
                    makeIntegerParam("minsplit", lower = 5, upper = 20),
                    makeIntegerParam("minbucket", lower = 3, upper = 10),
                    makeNumericParam("cp", lower = 0.01, upper = 0.1),
                    makeIntegerParam("maxdepth", lower = 3, upper = 10))
# Defining the random search
  randSearch  <- makeTuneControlRandom(maxit = 200)
  cvForTuning <- makeResampleDesc("CV", iters = 10)

  treeWrapper <- makeTuneWrapper("classif.rpart", resampling = cvForTuning,
                                  measures = list(f1,tpr,ppv,tnr),
                                  par.set = treeParamSpace,
                                  control = randSearch)
  cvWithTuning <- resample(treeWrapper, treeTask, resampling = outer,extract=getTuneResult)

  save(cvWithTuning, file=cvTuning)
}
gg <- getNestedTuneResultsOptPathDf(cvWithTuning)
#getNestedTuneResultsX(cvWithTuning)
```

```{r}
gg
```


\newpage
# Model Interpretability & Explainability

\newpage
# Conclusion

\newpage
# References {-}


1. Deborah Nolan; Duncan Temple Lang. Data Science in R.Chapman and Hall/CRC, 2015.
2. [The statistica report](https://www.statista.com/statistics/420391/spam-email-traffic-share/)
3. [The Paper I](https://www.ijser.org/researchpaper/EMAIL-SPAM-FILTERING-USING-DECISION-TREE-ALGORITHM.pdf) 	Email spam filtering using decision tree algorithm. 
4. [The Paper II](https://ijcat.com/archives/volume5/issue2/ijcatr05021004.pdf)	Identifying Valid Email Spam Emails Using Decision Tree. 


```{r}
# \donttest{
task = makeClassifTask(data = iris, target = "Species")
lrn = makeLearner("classif.rpart")
# stupid mini grid
ps = makeParamSet(
  makeDiscreteParam("cp", values = c(0.05, 0.1)),
  makeDiscreteParam("minsplit", values = c(10, 20))
)
ctrl = makeTuneControlGrid()
inner = makeResampleDesc("Holdout")
outer = makeResampleDesc("CV", iters = 2)
lrn = makeTuneWrapper(lrn, resampling = inner, par.set = ps, control = ctrl)
mod = train(lrn, task)
#> Error: Please use column names for `x`
print(getTuneResult(mod))
#> Error in checkClass(x, classes, ordered, null.ok): object 'mod' not found
# nested resampling for evaluation
# we also extract tuned hyper pars in each iteration
r = resample(lrn, task, outer, extract = getTuneResult)
getNestedTuneResultsOptPathDf(r)
```
 
 