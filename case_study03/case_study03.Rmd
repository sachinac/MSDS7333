---
title: 'MSDS 7333 Spring 2021: Case Study 03 '
author: "Sachin Chavan,Tazeb Abera,Gautam Kapila,Sandesh Ojha"
date: "`r format(Sys.time(), '%Y %B %d')`"
output:
  pdf_document:
    keep_tex: yes
    extra_dependencies: float
  word_document: default
  html_document:
    df_print: paged
subtitle: Email Spam Detection
header-includes:
- \usepackage{siunitx}
- \newcolumntype{d}{S[table-format=3.2]}
- \usepackage{multicol}  
- \usepackage{float}  
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[L]{Case Study 03}
latex_engine: pdflatex
urlcolor: gray    
---


   
```{r,echo=FALSE,message=FALSE,warning=FALSE}
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
library(kableExtra)
library(ggplot2)
library(purrr)
library(tidyr)
library(dplyr)
library(rpart)
library(rattle)
library(pander)
library(ggcorrplot)
library(caret)
library(ROSE)
library(naniar)
library(rpart.plot)
library(mlr)
source('src/cs03_methods.R')
theme_set(theme_bw())
```

```{r, echo=FALSE,cache=TRUE}
#lossmatrix <- matrix(c(0,1,2,0), byrow = TRUE, nrow = 2)
#mytree <- rpart(
#  isSpam ~ . , 
#  data = emailDFrp, 
#  method = "anova",
#  maxdepth = 5, 
#  minsplit = 2, 
#  minbucket = 1,
#  parms = list(loss = lossmatrix)
#)
#printcp(mytree)
#mytree <- prune(mytree, cp = 0.08)
#fancyRpartPlot(mytree, caption = NULL)
```

```{r, echo=FALSE,cache=TRUE}
load('data/data.Rda')
```

# Introduction

**E**mail spams are unsolicited emails that also referred as Junk emails sent in bulk. There is no law at least in United States that prevents anybody from sending unsolicited emails whether in single email or in bulk. Research indicate that email spams are accounted for over 80% of total email traffic.$^{[2][3][4]}$. First commercial spam incident was reported in 1994 $^{[5]}$. That email message was sent to 393 recipients advertising new model of VMS-DEC computer. There are different forms of email spams or unsolicited emails that may affect either individuals or organizations. Following are the few of them :

* Commercial Advertisements
* Hoax emails
* Emails Spoofs or phishing
* Lottery winining notification is very common
* Money Scams
* Virus/Malware
* Fills inbox
* Potentially steals private information
* Privacy concerns.

All these can adversely affect individual as well as organizations. Such emails comes from unknown sources that we never interacted with or sometimes spammers use spoofing techniques to mislead recipients to appear as valid source. Organizations also takes advantage to send bulk emails to potential customers to advertise their products.Such bulk emails occupies network traffic all the time. As far as personal email box goes human can easily categorize such emails and trash them manually. But nowadays there so many spams that it becomes hard to maintain email box without junk. That's where automated spam filters turns out to be very useful. Such a spam filters automatically detects spams and moves them to separate folder.Google for example and many other mailbox providers do run their own spam filters.

Now a days different machine learning algorithm are used for detecting and daily inspecting for incoming emails by different email provider companies like google, yahoo and the like. Among these machine learning algorithms decision tree methods are one of the best list.  

\newpage

# Business Understanding


The two common approaches used for filtering spam mails are knowledge engineering and machine learning. Emails are classified as either spam or ham using a set of rules in knowledge engineering $^{[4]}$. Machine learning approach have proved to be more efficient than knowledge engineering approach. No rule is required to be specified, rather a set of training samples which are pre-classified email messages are provided. A particular machine learning algorithm is then used to learn the classification rules from these email messages. 

Machine learning algorithms use statistical models to classify data. In the case of spam detection, a trained machine learning model must be able to determine whether the sequence of words found in an email are closer to those found in spam emails or safe ones.

Several studies have been carried out on machine learning techniques and many of these algorithms are being applied in the field of email spam filtering. Examples of such algorithms include Deep Learning, NaÃ¯ve Bayes, Decision tree, Support Vector Machines, Neural Networks, K-Nearest Neighbor, Rough sets, and Random Forests.

For this case study we are implementing a decision tree algorithm. Decision trees provides some unique advantages over other ML algorithms like Naive Bayes, SVM, kNN etc. Some of the aspect are expored as a part of current work. The approach uses a decision tree to go from observations associated with the email to target classification of being spam or not a spam. We explore a decision tree package in R called rpart, which is short for recursive partitioning. 


## Objective

Our objective is to investigate and optimize key hyperparameters used in the rpart package in order to classify email messages as spam or Ham email. In order to accomplish this, we fit a default decision tree and an optimized decision tree and compare them. 


\newpage

# Data Evaluation / Engineering

The dataset provided for this cases study was preprocessed and labeled already and contains features extracted from after preprocessing. It contains total of 9348 unique emails with 29 predictor variables and one response variable named isSpam. Of the 30 total variables, 17 are boolean factor variables and the remaining 13 variables are numeric variables. Each email has been previously classified as spam or valid. This dataset will be used to build a model using decision trees to classify email messages as spam or ham. Details of each feature is as follows:

## Factor variables

```{r, comment=NA,echo=FALSE,cache=TRUE}
factor_cols <- emailDFrp %>% keep(is.factor)  %>% colnames()
factor_desc <- c("Target Variable (T=Spam/F=Ham)",
                 "T=If subject starts with Re: F=Otherwise","T=If FROM email address field contains underscore F=Otherwise",
                 "T=If priority key present in the header F=Otherwise","T=If inReplyTo present in the header F=Otherwise",
                 "T=If Recipient's email addresses are sorted F=Otherwise","T=if Subject line contains punctuation F=Otherwise",
                 "T=If MIME type is multipart/text F=otherwise","T=If email body contains images F=Otherwise",
                 "T=If message contains PGP signature F=Otherwise","T=If subject contains one of the words from spam vector F=Otherwise",
                 "T=If there is no hostname in header F=Otherwise","T=If sender's email address ends with number",
                 "T=If subject line contains all uppercase letters F=Otherwise","T=If Message body contains word original message F=Otherwise",
                 "T=If word dear found in message body F=Otherwise","T=if message body contains word Wrote: F=Otherwise"
                 )

text_tbl <- data.frame(
Feature = factor_cols,
Description = factor_desc
)
kbl(text_tbl, booktabs = T) %>%
kable_styling(full_width = F) %>% column_spec(2, width = "30em")
```

## Numeric variables

```{r, comment=NA,echo=FALSE,cache=TRUE}
numeric_cols <- emailDFrp %>% keep(is.numeric)  %>% colnames()
numeric_desc <- c("Number of Lines in the message","Number of characters in the messsage ",
                 "Number of Exclamation in Subject line","Number of question marks in subject line",
                 "Number of email attachments","Number of Recipients in the email message",
                 "Percentage of uppercase letters in the Body of the message","Hour of the day",
                 "Percentage of HTML tags in the body of the message","Percentage blanks in Subject line",
                 "Number of consecutive forward symbols in the body of the message","Average length of the word in the body of the message",
                 "Number of dollar symbol in the message body"
                 )
text_tbl <- data.frame(
Feature = numeric_cols,
Description = numeric_desc
)
kbl(text_tbl, booktabs = T) %>%
kable_styling(full_width = F) %>% column_spec(2, width = "30em")
```

## Structure of dataframe

As shown below this dataframe contains 30 columns and 9348 observations.

```{r,comment=NA, echo=FALSE,cache=TRUE}
str(emailDFrp,vec.len=3)
```

## Summary of Factor Variables
```{r,comment=NA, echo=FALSE,cache=TRUE}
emailDFrp %>% keep(is.factor) %>% summary()
```

## Summary of Numeric Variables

```{r,comment=NA, echo=FALSE,cache=TRUE}
emailDFrp %>% keep(is.numeric) %>% summary()
```

Summary shows missing values in subSpamWords(7),noHost(1),isYelling(7) from factor variables and in subExcCt(20), subQuesCt(20), numRec(282),subBlanks(20) from numeric fields

## Missing Values : Visualization

```{r, comment=NA,include=TRUE, cache=TRUE, echo=FALSE, warning=FALSE,message=FALSE, fig.cap=cap,fig.width=6,fig.height=5,fig.align='center',fig.fullwidth=FALSE}
gg_miss_var(emailDFrp,show_pct = FALSE,facet=isSpam)+
  labs(y = "All Missing at once ")
cap <- "Left : Not Spam, Right : Spam"
```

Interpretation: 

* noHost is missing on single record when all other values are present
* subSpamWords & isYelling are missing on same 7 records when all other values are present
* subExcCt, subQuesCt, subBlanks appears to be missing on same 20 records
* It is observed that 13 of 20 rows in subBlanks are NaN.
* numRec has missing values on 282 records most likely due to recipients were added in BCC and only 12 emails have been flagged as Spam and remaining 270 are not spam. These 282 missing values are appears to be random and satisfies MAR (Missing at Random). Missing values will be fixed using mean imputation for both categories (Spam and not Spam) seperately.

## Missing Values : Assumptions and Actions

Instead of deleting records with missing values following actions will be taken on the missing data. Imputation will be applied to numRec variable.

```{r, comment=NA,echo=FALSE,cache=TRUE}
FeatureName <- c("noHost","subSpamWords","isYelling", "subExcCt", "subQuesCt", "numRec","subBlanks")
DataType    <- c("factor","factor","factor","numeric","numeric","numeric","numeric")
Missing     <- c(1,7,7,20,20,282,20)
Assumption  <- c("HostName missing","Subject line Missing","Subject Line Missing",
                "Subject Line without exclamation mark","Subject Line without question mark","Undisclosed recipients","Subject Line Witout Blanks")
Action      <- c("Replace NAs with F","Replace NAs with F","Replace NAs with F",
                 "Replace NAs with 0s","Replace NAs with 0s","Apply mean imputation to Spam and Not Spam groups seperately","Replace NAs with 0s")

text_tbl <- data.frame(
FeatureName = FeatureName,
DataType = DataType,
Missing = Missing,
Assumption = Assumption,
Action = Action
)

kbl(text_tbl, booktabs = T) %>% kable_styling(full_width = F) %>% column_spec(5, width = "15em")
email_df <- emailDFrp 

email_df[c(which(is.na(email_df$subSpamWords))),]$subSpamWords <- as.factor("F")
email_df[c(which(is.na(email_df$noHost))),]$noHost             <- as.factor("F")
email_df[c(which(is.na(email_df$isYelling))),]$isYelling       <- as.factor("F")

email_df[c(which(is.na(email_df$subExcCt))),]$subExcCt   <- 0
email_df[c(which(is.na(email_df$subQuesCt))),]$subQuesCt <- 0
email_df[c(which(is.na(email_df$numRec))),]$numRec       <- 0
email_df[c(which(is.na(email_df$subBlanks))),]$subBlanks <- 0
```

## No Missing data in Final DataFrame

```{r, comment=NA,echo=FALSE,cache=TRUE, fig.width=6,fig.height=4, fig.align='center',fig.cap=cap}
gg_miss_var(email_df,show_pct = FALSE,facet=isSpam)+
  labs(y = "All Missing at once ")
cap <- "Left : Not Spam, Right : Spam"
```

### Class Distribution (Target Variable)

Below chart shows target variable is binary and class lables are not evenly distributed. This is the case of imbalanced dataset.

```{r, comment=NA,echo=FALSE,cache=TRUE, fig.width=2,fig.height=2, fig.align='center',fig.cap="Target Variable"}
bar <- modified_df(email_df)

g <- ggplot(bar,aes(x=isSpam,y=Total))
g +  geom_bar(stat="identity", width = 0.4, fill="tomato2") + 
     labs(title="Spam vs No Spam" ) +
     xlab("Email Spams")+
     theme(plot.title = element_text(hjust = 1.2))+
     ggthemes::theme_excel_new()
```


```{r, include=TRUE, echo=FALSE, cache=TRUE,out.align='left',fig.pos="top",results='markup',fig.cap="Target Percentage Distribution"}
#pandoc.table(bar,stype="rmakrdown",justify="left")
kbl(bar, caption="Target Percentage Distribution",booktabs = T) %>% kable_styling(full_width = F,latex_options = c("hold_position")) 
```

### Correlation Plot

This plot shows numLines and bodyCharCt are strongly correlated at 90% and one of them can be removed. So bodyCharCt will be removed from further analysis.

```{r, comment=NA,echo=FALSE,cache=TRUE, fig.width=5,fig.height=4, fig.align='center',fig.cap="Correlogram"}
corr <- round(cor(email_df[,c(18:30)]), 1)
#ggcorrplot(corr, hc.order = TRUE, 
#           type = "lower", 
#           lab = TRUE, 
#           lab_size = 2, 
#           method="circle", 
#           colors = c("tomato2", "white", "springgreen3"), 
#           title="Correlogram of Numeric Features", 
#           ggtheme=theme_bw)

ggcorrplot(corr, hc.order = TRUE, type = "lower",
   outline.col = "white",
   ggtheme = ggplot2::theme_gray,
   colors = c("#6D9EC1", "gray", "#E46726"))

email_final <- email_df[,c(1:18,20:30)]
```

## Strcuture of Final DataFrame
```{r, include=TRUE, echo=FALSE, cache=TRUE,out.align='left',fig.pos="top",results='markup'}
str(email_final)
```

## Final Dataframe for analysis 

Structure of DataFrame has not changed. In previous steps only missing values have been replaced with 0s or Fs based on assumption made. Here is sample DataFrame.

```{r, include=TRUE, echo=FALSE, cache=TRUE,out.align='left',fig.pos="top",results='markup'}
knitr::kable(email_final[1:7,c(1:3,18:24)], caption="DataFrame", row.names = FALSE,format = "latex",position = "!b")
```

## Train/Test split

```{r, comment=NA,echo=FALSE,cache=TRUE}
# Split the data into training and test set
set.seed(123)
training.samples <- email_final$isSpam %>%  
                    createDataPartition(p = 0.8, list = FALSE)
train.data <- email_final[training.samples, ]
test.data  <- email_final[-training.samples, ]
```

Original data to be split into train(0.8) and test set (0.2). R package caret provides library function createDataPartition to split data into train/test set and it performs stratified sampling by default as per its documentation. Stratified sampling technique preserves percentage of samples of each class. Target variable in this case is binary i.e. email could either be Spam or not Spam and only approximately 25% emails have been labeled as Spam therefore Stratified sampling is the best way to proceed.

### Train - Target Percentage Distribution


**createDataPartition** function from **caret** packages was used to split data into train/test set.Target variable is binary and distribution of classes is not balanced. i.e. 70% observations are not spam and only 30% are labeled as Spam in this dataset. This is case of imbalanced dataset. Random Stratified sampling has been used to split data into train/test set for building model using decision trees. **Stratified Sampling** preserves the percentage of samples of each class. Scaling and normalization is required for decision tree models.

```{r, include=TRUE, echo=FALSE, cache=TRUE,out.align='left',fig.pos="top",results='markup'}
bar1 <- modified_df(train.data)
kbl(bar1, caption="Target Percentage Distribution",booktabs = T) %>% kable_styling(full_width = F,latex_options = c("hold_position") )
```

## Constraints

 Following are limitations of Decision tree algorithms: 
 
* Decision Tree algorithms are greedy algorithms, prone to overfitting.If not handled properly tree tends to grow larger and become complex and difficult to interpret. Overfitting affects generalizability and hence accuracy of prediction goes down significantly.
* Trees are unstable, meaning that even small change in information leads to big change in trees.
* Loses lot of information in splitting process.
* Bad for large datasets

However, there are different ways to tackle these problems. Hyperparameter tunining, building stopping criteria are discussed in next sections.

\newpage

# Modeling Preparations

## Approaches:  

Two approaches were under consideration for email SPAM classification problem, (1) NaÃ¯ve Bayes, and (2) decision tree (DT) based approach. These are discussed below and a case is made for use of decision tree for modeling problem. NaÃ¯ve Bayes (NB) requires or assumes an underlying probabilistic model. This is not a requirement for decision tree. In contrast to NB approach, decision tree algorithms are relatively easier to understand in terms of understanding classification algorithm, and also provides easy identification of important features, and handling of missing values. In DT based approach it is easier to accommodate additional derived features from an email (e.g. number of forwards, number of capital letters in subject line, etc.), and not just the word content. To explore and capture above, DT based approach is pursued in this work.

## Classification Metrics: 
For the binary classification problem at hand, with goal of predicting if Email is SPAM or NOT, we have following confusion matrix interpretation:



While accuracy is a default metric for better classification, for current problem we need to discuss importance of FP (False Positive) and FN (False Negative) scenarios to identify metrics:

**False Positive (FP) rate needs to be minimized.** 

* It is the number of times Email is classified as SPAM, while actually it was not. 
* Goal is that the SPAM filter should not automatically block emails by classifying them as SPAM, when they actually are not. Otherwise, users could end up loosing a lot of important emails. 
* This implies that specificity given by $\frac{TN}{TN+FP}$ = ~1, or as high as possible.

**False Negative (FN) rate should be minimized, but is less critical than FP rate.**

*	This is the number of times Email is classified as Ham, while actually it was SPAM.
* FN rate is less critical than FP rate, as we can occasionally allow SPAM messages to reach end user, as long as we get better model, i.e. a very low FP rate. 
* It is assumed that that the end person can exercise judgement and categorize such emails as SPAM, and take corrective actions as per companyâs IT policy.


Above arguments lead us to select following metrics to be considered while choosing the best model:

* Specificity, or True Negative Rate (TNR)

  * Given $TNR=\frac{TN}{TN+FP}$
  * Minimizing FP, implies we need to select model with highest TNR values

* Precision, or Positive Predictive Value (PPV)  

  * Given $PPV=\frac{TP}{TP+FP}$ 
  * Minimizing FP, implies we need to select model with highest PPV values
  
There is additional flexibility in choosing model with a slightly poorer recall, or sensitivity $\frac{TP}{TP+FN}$ as cost of FP is larger than FN, and focus is minimizing FP over FN.

Lastly, we have an imbalanced classification problem at hand. Refer Figure <XYZ> above, that shows that ~75% outcomes are HAM, while ~25% outcomes are SPAM in modeling dataset. Due to above, and to provide a better balance between precision and recall, F1 metric is additionally chosen as a metric for choosing the best model. F1 is harmonic mean of âPrecisionâ and âRecallâ and given by


$\frac{2*Precision*Recall}{Precision+Recall}$
                                                          
F1 score penalizes if any one of precision or recall falls low, which may also happen due to imbalanced classification problem at hand when attempting k-fold cross validation.
Basic Tree Model & Parameters

Parameters tuned for our basic tree are mentioned below along with description:
minsplit	- the minimum number of observations that must exist in a node in order for a split to be attempted
minbucket	- the minimum number of observations in any terminal <leaf> node
cp 	- complexity parameter. Any split that does not decrease the overall lack of ï¬t by a factor of cp is not attempted. A high cp value implies a split is attempted only if information gain is larger by amount cp. Choosing a higher value of cp reduces number of tree splits, as fewer splits will meet the high bar on information gain as set by high cp parameter.

In our analysis above parameters are varied.



\newpage
# Model Building & Evaluation

Decision tree algorithm are prone to overfit quickly and loses generalizability for prediction i.e. prediction performance become poor. Solution to this problem is hyperparameter tuning. Decision trees have several hyparameters that can be tuned to get optimal model. Below table lists important hyperparameters of which minsplit,minbucket, cp and maxdepth will be used here to tune the model. These are the stopping criterion for the trees which can be applied at each stage during tree-building process.


```{r, include=TRUE, echo=FALSE, cache=TRUE,out.align='left',fig.pos="top",results='markup'}
hyper_parameter <- c("minsplit","minbucket","cp","maxdepth", "maxsurrogate","usesurrogate","surrogatestyle")
hp_desc      <-    c("Minimum number of observations required to split the given node. If it is set to 20 indicates that if data has less than 20 records then root node becomes leaf
                   node.","Minimum number of observations required at leaf node. default is minbucket = minsplit/3",
                   "Complexity Parameter to control size of the tree and also known as cost of adding another variable to decision tree",
                   "Set the maximum depth of any node of the final tree",
                   "Used if data contains missing values. useful to determine which split can be used for missing data",
                   "Controls use of surrogate split",
                   "Controls selection of best surrogate"
                 )
text_tbl <- data.frame(
HyperParameters = hyper_parameter,
Description = hp_desc
)
kbl(text_tbl, booktabs = T) %>%
kable_styling(full_width = F) %>% column_spec(2, width = "30em")
```

*getParamSet(tree)* function from **R package mlr** returns following list of hyper-parameters for the decision tree.  

```{r, comment=NA,include=TRUE, echo=FALSE, cache=TRUE,out.align='left',fig.pos="top",results='markup'}
# Creating the task and learner
treeTask <- makeClassifTask(data = train.data, target = "isSpam")
tree <- makeLearner("classif.rpart")
getParamSet(tree)
```

**Tuned Parameters**

Parameters are tuned with following criteria
  
*  5 <= minsplit  <=20
*  3 <= minbucket <=10
*  3 <= maxdepth  <=10
*  0.01 <= cp <=0.1
*  10 fold cross validation
*  Random search Max iterations 200

Here are tuning results:
 
```{r, comment=NA,include=TRUE, message=FALSE, echo=FALSE, warning=FALSE,cache=TRUE,out.align='left',fig.pos="top",results='markup'}

# Defining the hyperparameter space for tuning
tunedTree <- "data/TunedTreeParms.dat"
if (file.exists(tunedTree)) {
  load(file=tunedTree)
} else {
treeParamSpace <- makeParamSet(
  makeIntegerParam("minsplit", lower = 5, upper = 20),
  makeIntegerParam("minbucket", lower = 3, upper = 10),
  makeNumericParam("cp", lower = 0.01, upper = 0.1),
  makeIntegerParam("maxdepth", lower = 3, upper = 10))

# Defining the random search
randSearch  <- makeTuneControlRandom(maxit = 200)
cvForTuning <- makeResampleDesc("CV", iters = 10)

tunedTreePars <- tuneParams(tree, task = treeTask,
                            measures = list(f1,tpr,ppv,tnr),
                            resampling = cvForTuning,
                            par.set = treeParamSpace,
                            control = randSearch)
save(tunedTreePars, file=tunedTree)
}
tunedTreePars
```


```{r, comment=NA,include=TRUE, message=FALSE, echo=FALSE, warning=FALSE,cache=TRUE,out.align='left',fig.pos="top",results='markup'}
# Cross-validating the model-building process
cvTuning <- "data/cvWithTuning.dat"
if (file.exists(cvTuning)) {
  load(file=cvTuning)
} else {
  outer <- makeResampleDesc("CV", iters = 10)
  
  treeParamSpace <- makeParamSet(
                    makeIntegerParam("minsplit", lower = 5, upper = 20),
                    makeIntegerParam("minbucket", lower = 3, upper = 10),
                    makeNumericParam("cp", lower = 0.01, upper = 0.1),
                    makeIntegerParam("maxdepth", lower = 3, upper = 10))
# Defining the random search
  randSearch  <- makeTuneControlRandom(maxit = 200)
  cvForTuning <- makeResampleDesc("CV", iters = 10)

  treeWrapper <- makeTuneWrapper("classif.rpart", resampling = cvForTuning,
                                  measures = list(f1,tpr,ppv,tnr),
                                  par.set = treeParamSpace,
                                  control = randSearch)
  cvWithTuning <- resample(treeWrapper, treeTask, resampling = outer,extract=getTuneResult)

  save(cvWithTuning, file=cvTuning)
}
gg <- getNestedTuneResultsOptPathDf(cvWithTuning)
#getNestedTuneResultsX(cvWithTuning)

```

**Impact of hyper parameter tuning on model's performance**

Model performance metric variation as a function of tuning parameters is analyzed through plots below.
 
_Impact due to Complexity Parameter (cp)_

Complexity parameter controls the tree growth, and additional splits at a level are attempted only if information gain is larger by amount cp. 

* Higher cp value puts a larger requirement on information gain with every split, leading to smaller trees. 
* Smaller cp values lower thre requirement on information gain with every split, leading to larger trees. 

As can be seen from figure below:

* Smaller cp values, lead to better or improved metric (F1 score)
* For maxdepth upto 4, reducing cp does not improve the metric, which is intuitive, as tree stops to grow. However increasing maxdepth > 4 allows better metric.

Clearly maxdepth and cp directly help control and influence the classification model metrics.

```{r, comment=NA,include=TRUE, message=FALSE, echo=FALSE, warning=FALSE,cache=TRUE, fig.width=5,fig.height=4, fig.align='center',results='markup'}

ggplot(data=gg)+
  geom_point(aes(x = cp, y=f1.test.mean, color = factor(maxdepth)))+
  # scale_color_grey()+
  theme_light()+
  theme(plot.title = element_text(hjust = 0.5),panel.grid.minor = element_line(colour="grey", size=0.1),panel.grid.major = element_line(colour="grey", size=0.3))+
  scale_x_log10()+
  labs(title="F1 Score Vs. Complexity Parameter (cp)",x="Complexity Parameter (cp)", y = "F1 Score",col = "maxdepth")
```

_Impact due to Minsplit and Minbucket_

Minsplit is the minimum number of observations that must exist in a node in order for a split to be attempted.

* Smaller value is expected to lead to tree growth.

Minbucket is the  minimum number of observations in any terminal <leaf> node

* Smaller value is expected to lead to tree growdth

However, Minsplit and Minbucket oppose each other sometimes, and may render other value redundant. E.g. minsplit has to be larger than minbucket for both of the parameters to operate independently.

Looking at impact of Minsplit and Minbucket on F1 metric, refer figure below, does not indicate that they play a major role in determining model score like F1.  It is clear that cp value is what drives the model performance.

```{r, comment=NA,include=TRUE, message=FALSE, echo=FALSE, warning=FALSE,cache=TRUE, fig.width=5,fig.height=4, fig.align='center',results='markup'}

ggplot(data=gg)+
  geom_point(aes(x = minbucket, y=f1.test.mean, color = cp))+
  # scale_color_grey()+
  theme_light()+
  theme(plot.title = element_text(hjust = 0.5),panel.grid.minor = element_line(colour="grey", size=0.1),panel.grid.major = element_line(colour="grey", size=0.3))+
  scale_color_gradient(low = "blue", high = "red")+
  scale_x_log10()+
  labs(title="F1 Score Vs. Minbucket",x="Minbucket", y = "F1 Score")
```

```{r, comment=NA,include=TRUE, message=FALSE, echo=FALSE, warning=FALSE,cache=TRUE,fig.width=5,fig.height=4, fig.align='center',results='markup'}

ggplot(data=gg)+
  geom_point(aes(x = minsplit, y=f1.test.mean, color = cp))+
  # scale_color_grey()+
  theme_light()+
  theme(plot.title = element_text(hjust = 0.5),panel.grid.minor = element_line(colour="grey", size=0.1),panel.grid.major = element_line(colour="grey", size=0.3))+
  scale_color_gradient(low = "blue", high = "red")+
  scale_x_log10()+
  labs(title="F1 Score Vs. Minsplit",x="Minsplit", y = "F1 Score")

```

**Model Performance of Tuned Model**

Tuned decision tree has following hyperparamter values:

* minsplit  = 8
* minbucket = 3 
* cp        = 0.0125
* maxdepth  = 9

Below model performance summary captures all metrics associated with classification task.

```{r, comment=NA,include=TRUE, message=FALSE, echo=FALSE, warning=FALSE,cache=TRUE,out.align='left',fig.pos="top",results='markup'}

model = rpart(formula=isSpam~.,
              data=train.data,
              method="class",
              parms = list(split = "information"),
              cp = 0.0125,                           # no size penalty
              maxdepth = 9,                         # defines the maximum depth a tree can grow
              minsplit = 8,                         # Nodes of size 5 (or # more) can be split,
              minbucket = 3,                        # provided each sub-node contains at least 2 ob
             )

predTest = predict(object=model, newdata=test.data, type="class")
cm <- confusionMatrix(table(predTest,test.data$isSpam)[c(2,1),c(2,1)],mode = "everything")

print (cm)
```

_Metrics used to evaluate the model are:_

* Precision     : 0.8365
* Specificity   : 0.9504
* F1 score      : 0.7836
* Recall        : 0.7370
* Accuracy      : 0.8957

_Observations_

* Model has good Specificity and Precision, indicative of smaller False Positive which is desired.
* The F1 score shows a good balance between precision and recall.



```{r, comment=NA,include=TRUE, message=FALSE, echo=FALSE, warning=FALSE,cache=TRUE,out.align='left',fig.pos="top",results='markup'}
# Exploring the model
#@printcp(treeModelData, digits = 3)
```
\newpage
# Model Interpretability & Explainability

Most important features from the model that was built after 10 fold cross validation and hyperparameter tuning is as shown below. While building model using decision trees assigns scores to these variables. Feature scores gives us clear picture which features tree found most important. So for this particular model we can clearly see  feature perCaps (percentatge uppercase letters ) is the most important feature followed by numLines (number of lines per message),perHTML(percentage HTML tags),numDlr (Number of dollar symbol). As per this model perCaps is the most deciding factor to know if email is spam or not then number of lines and percentage of HTML tags.

```{r, comment=NA,include=TRUE, message=FALSE, echo=FALSE, warning=FALSE,fig.cap="Feature importance"}

# Training the final tuned model
tunedTree <- setHyperPars(tree, par.vals = tunedTreePars$x)
tunedTreeModel <- train(tunedTree, treeTask)

# Plotting the decision tree
treeModelData <- getLearnerModel(tunedTreeModel)


dat <- data.frame(vars=names(treeModelData$variable.importance ), 
                  importance=treeModelData$variable.importance )

dat1 <- dat  %>% arrange(importance)
dat1$vars <- factor(dat1$vars, levels =dat1$vars) 

# Draw plot
ggplot(dat1, aes(x=vars, y=importance)) + 
  geom_bar(stat="identity", width=.5, fill="tomato3") + 
  coord_flip()+
  labs(title="Feature importance", 
       subtitle="Decision Trees", 
       caption="source: unknown") + 
  theme(axis.text.x = element_text(angle=65, vjust=0.6))


```


The most importatn feature is perCaps (Percentage capital letters in the body of the message) and least important feature is isPGPsigned (Signature). perCaps contributes more than any other features. Almost 600 emails have been flagged as a Spam while numLines contributes little less than 400 towards Spams out of total emails. This is quite what we see on daily basis all such Spams contains capital letters. This bar plot is quite intuative and self explanatory.

```{r, comment=NA,include=TRUE, message=FALSE, echo=FALSE, warning=FALSE,fig.cap="Decision Tree"}
rpart.plot(treeModelData, roundint = FALSE,
box.palette = "BuBn",
type = 3)

```

Interpreting trees is quite straightforward. One just need to answer series of questions to go to bottom of the tree to classify message as Spam or not Spam. 

**Example 1**

Here is one example randomly picked up from the dataset and let's walkthrough the tree from top to bottom of the tree by answering questions.

```{r, comment=NA,include=TRUE, message=FALSE, echo=FALSE, warning=FALSE,fig.cap="Decision Tree"}
sqldf::sqldf("select isSpam,perCaps,numLines,perHTML,numDlr,forwards,isInReplyTo,isRe  from email_final where isSpam='T' limit 1" )
```

Q1 - Is perCaps > 13 

A1 - Yes PerCaps = 38.47 > 13 move to right branch

Q2 - is numLines > 9.5  

A2 - Indeed numLines = 101 >9.5 move to right branch

Q3 - is isInReplyTo T or F

A3 - its False go to the right branch and  this is leaf node.

By answering just three questions we concluded that email is Spam email. This is real advantage of decision trees. They superbly interpretable compared to other models.


**Example 2**

Here is another example for email that is not a spam.

```{r, comment=NA,include=TRUE, message=FALSE, echo=FALSE, warning=FALSE,fig.cap="Decision Tree"}
sqldf::sqldf("select isSpam,perCaps,numLines,perHTML,numDlr,forwards,isInReplyTo,isRe  from email_final where isSpam='F' limit 1" )
```


Q1 - Is perCaps < 13 

A1 - Yes perCaps= 4.45 < 13  move towards left Subtree 

Q2 - Is perHTML < 3.9 

A2 - Yes perHTML=0 < 3.9 move towards left Subtree

Q3 - Is forwards > 1.2 

A3 - No forwards=0 < 1.2 move towards right Subtree

Q4 - Is isInReplyTo = T ?

A4 - isInReplyTo = T move towards left subtree and this is leaf node which tells us that this email is not a Spam.

Again by answering just four questions we concluded that email is not a Spam. 

\newpage
# Conclusion

An Email SPAM filtering model is developed that is capable of accurately identifying SPAM 90% of time, and with a true negative rate, or specificity at 95%. This implies very few times a non-SPAM email gets classified as SPAM.

A decision tree based classification approach is developed that can be deployed alongside automatic reading of email and doing a real time check to categorize email as a SPAM or not SPAM.

The model used following variables for classification of email in SPAM:

* forwards 
* perCaps
* perHTML
* numLines
* numDlr
* subBlanks
* isInReplyTo

An email is classifies as SPAM when any of following rule is met:

* perCaps >= 13 ,  and numLines > 9.5, and, isInReplyTo is false 
* perCaps < 13  ,  and perHTML > 3.9,  and numLines > 242
* perCaps < 13  ,  and perHTML < 3.9,  and  #forwards < 1.2., and numDlr >= 0.5, isInReplyTo is false, and, numLines < 103
* perCaps < 13  ,  and perHTML < 3.9,  and  #forwards < 1.2., and numDlr < 0.5 , isInReplyTo is true,  and, subBlanks >=25 
* perCaps < 13  ,  and perHTML < 3.9,  and  #forwards < 1.2., and numDlr < 0.5 , isInReplyTo is true,  and, subBlanks < 25, and subExcCt >=0.5 and numLines > 17
* perCaps < 13  ,  and perHTML < 3.9,  and  #forwards < 1.2., and numDlr < 0.5 , isInReplyTo is true,  and, subBlanks < 25, and subExcCt < 0.5 and isDear = 2

Decision trees are super interpretable compared to other models/algorithms, For future work Naive Bayes, SVM, Stockastic models could also be explored and benchmarked for model performance. 


\newpage
# References {-}


1. Deborah Nolan; Duncan Temple Lang. Data Science in R.Chapman and Hall/CRC, 2015.
2. [The statistica report](https://www.statista.com/statistics/420391/spam-email-traffic-share/)
3. [The Paper I](https://www.ijser.org/researchpaper/EMAIL-SPAM-FILTERING-USING-DECISION-TREE-ALGORITHM.pdf) 	Email spam filtering using decision tree algorithm. 
4. [The Paper II](https://ijcat.com/archives/volume5/issue2/ijcatr05021004.pdf)	Identifying Valid Email Spam Emails Using Decision Tree. 
5. [Connection for Businesss website](https://www.connections.com/blog/the-negative-effects-of-spam)

