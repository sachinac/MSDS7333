{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "secret-pitch",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <font size=5> <b> MSDS 7333 Spring 2021: Case Study 06 </b></font>\n",
    "</div>\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <font size=3> <b> Classification  Using Neural Networks </b> </font>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <font size=3> <b> Sachin Chavan,Tazeb Abera, Gautam Kapila, Sandesh Ojha </b> </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-confidentiality",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Modules\" data-toc-modified-id=\"Import-Modules-1\">Import Modules</a></span></li><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-2\">Introduction</a></span></li><li><span><a href=\"#Business-Understanding\" data-toc-modified-id=\"Business-Understanding-3\">Business Understanding</a></span></li><li><span><a href=\"#Data-Exploration-&amp;-Quality\" data-toc-modified-id=\"Data-Exploration-&amp;-Quality-4\">Data Exploration &amp; Quality</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-4.1\">Load Data</a></span></li><li><span><a href=\"#DataFrame\" data-toc-modified-id=\"DataFrame-4.2\">DataFrame</a></span></li><li><span><a href=\"#More-on-Data-Quality\" data-toc-modified-id=\"More-on-Data-Quality-4.3\">More on Data Quality</a></span></li><li><span><a href=\"#Sampled-Data\" data-toc-modified-id=\"Sampled-Data-4.4\">Sampled Data</a></span></li></ul></li><li><span><a href=\"#Architecture\" data-toc-modified-id=\"Architecture-5\">Architecture</a></span></li><li><span><a href=\"#Recommendation-&amp;-Evaluations\" data-toc-modified-id=\"Recommendation-&amp;-Evaluations-6\">Recommendation &amp; Evaluations</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-7\">Conclusion</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-8\">References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-brain",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "danish-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-aaron",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This case study is about using neural networks for classification problem with the Higgs Boson dataset. The dataset that contains observations of kinematic properties measured by particle detectors in accelerator [[1]](#References).Higgs Boson is the unusal kind of subautomic particle discovered by Scottish physicist Peter Higgs [[2]](#References). The mainstream media always referred this particle as God particle after launch of a book The God particle by Leon Lederman. This particles are produced by quantum excitation of Higgs field [[2]](#References).In 1964, Peter Higgs proposed a mechanism to explain why some particle have mass. There are several ways by which particle can attain a mass like by interacting with Higgs field. Higgs field exist just like gravitational or magnetic field that doesn't change [(P. Onyisi,2013)](#References). The experiment was carried out in year 2012 by ATLAS and CMS and they found subautomic particle have properties similar to explained by Higgs mechanism.\n",
    "\n",
    "The dataset for this case study contains 11 million observations produced using Monte Carlo simulations and has observations of singal processes that produces Higgs Boson and background process that does not and shall be used to distinguish between signal processes and background process.So this is signal vs background classification problem.\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-denmark",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "Deep learning is a new area in Machine Learning that attempts to model high level abstractions present in the raw data to understand the high varying functions underlying the data and to perform well generalized predictions for unseen data. This is accomplished through certain non-linear transformations of data through varying deep architectures such as Neural Networks. Deep learning aims at fulfilling the objective of true Artificial Intelligence and has recently been of great interest to researchers in machine learning. Tech giants like Google, Microsoft, Facebook and Baidu are investing hundreds of millions of dollars in bleeding-edge deep learning research and developing its applications [[2]](#References).\n",
    "\n",
    "As described in the paper (Baldi,2014) collisions at high energy particles are great source of finding exotic particles, basically these are classification problems and requires machine learning. Classical machine learning have limitations in learning non-linear patterns in the data and often requires features to be manually crafted and process is quite time consuming.However, Deep learning approach found to be more effective in reading and learning such non-linear structures and classifiying signals vs background processs more effectively compared to classical machine learning methods and that too witout manually crafting features as it is done in other modeling techniques.\n",
    "\n",
    "This field deals with fast and high energy collision of particles and how they form and decay. The formation and decay of these Higgs Boson can be observed and recorded. In the paper “Searching for Exotic Particles in High-Energy Physics with Deep Learning,” published in 2014, the authors used deep learning with neural networks to classify “exotic particle(s)” that results in particle collision. The Higgs Boson data was collected from the Large Hadron Collider’s detectors. In the paper above, they focused on improving past research that used other machine learning techniques. The deep learning method they used was built using the library standards in 2014 and it outperformed the previous methods. The model was implemented using Pylearn2 which is no longer supported.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Objective**\n",
    "\n",
    "Build a neural network model similar to that implemented in the paper (Baldi,2014) using TensorFlow to effectively classify signal and background processes to find exotic particles to prove that NN outperforms other techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-dayton",
   "metadata": {},
   "source": [
    "# Data Exploration & Quality\n",
    "\n",
    "The dataset was obtained from UCI website[[1]](#References). It contains data that was produced usign Monte Carlo simulations. Out of 21 features first 21 after target variables are low level kinematic properties measured by particle detectors and remaining 7 featuresa are function of 21 features.These are high level features as described on the UCI website. These are derived by physicists for the purpose of classification specially for deep learning. This eliminates the need for manually crafting feautre for data modeling. This data does not contain any missing values as indicated by the website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-stocks",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Here 50 records are randomly sampled from this huge dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "worst-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of records in file (excludes header)\n",
    "#Random sample of 50K records from the dataset \n",
    "random.seed(10)\n",
    "filename = \"data/HIGGS.csv\"\n",
    "n = sum(1 for line in open(filename)) - 1 \n",
    "s = 50000 \n",
    "skip = sorted(random.sample(range(1,n+1),n-s)) \n",
    "higgs_ds = pd.read_csv(filename,header=None,skiprows=skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-pregnancy",
   "metadata": {},
   "source": [
    "## DataFrame\n",
    "\n",
    "* First column is target variable 1 for signal and 0 for background followed by 28 features.\n",
    "* next 21 features are low level features\n",
    "* remaining 7 features are high level features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "opened-collaboration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>lepton_pT</th>\n",
       "      <th>lepton_eta</th>\n",
       "      <th>lepton_phi</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>missing_energy_phi</th>\n",
       "      <th>jet_1pt</th>\n",
       "      <th>jet_1eta</th>\n",
       "      <th>jet_1phi</th>\n",
       "      <th>jet_1b-tag</th>\n",
       "      <th>...</th>\n",
       "      <th>jet_4eta</th>\n",
       "      <th>jet_4phi</th>\n",
       "      <th>jet_4b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.869293</td>\n",
       "      <td>-0.635082</td>\n",
       "      <td>0.225690</td>\n",
       "      <td>0.327470</td>\n",
       "      <td>-0.689993</td>\n",
       "      <td>0.754202</td>\n",
       "      <td>-0.248573</td>\n",
       "      <td>-1.092064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010455</td>\n",
       "      <td>-0.045767</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>1.353760</td>\n",
       "      <td>0.979563</td>\n",
       "      <td>0.978076</td>\n",
       "      <td>0.920005</td>\n",
       "      <td>0.721657</td>\n",
       "      <td>0.988751</td>\n",
       "      <td>0.876678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>1.470639</td>\n",
       "      <td>-1.635975</td>\n",
       "      <td>0.453773</td>\n",
       "      <td>0.425629</td>\n",
       "      <td>1.104875</td>\n",
       "      <td>1.282322</td>\n",
       "      <td>1.381664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.128848</td>\n",
       "      <td>0.900461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909753</td>\n",
       "      <td>1.108330</td>\n",
       "      <td>0.985692</td>\n",
       "      <td>0.951331</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.865924</td>\n",
       "      <td>0.780118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.367665</td>\n",
       "      <td>-0.881496</td>\n",
       "      <td>0.232349</td>\n",
       "      <td>1.258956</td>\n",
       "      <td>1.728993</td>\n",
       "      <td>0.726262</td>\n",
       "      <td>0.981292</td>\n",
       "      <td>-0.303185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.871726</td>\n",
       "      <td>-0.460841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.806875</td>\n",
       "      <td>0.757239</td>\n",
       "      <td>0.987123</td>\n",
       "      <td>0.881511</td>\n",
       "      <td>0.625056</td>\n",
       "      <td>0.869624</td>\n",
       "      <td>1.198134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.610152</td>\n",
       "      <td>-0.720791</td>\n",
       "      <td>0.047025</td>\n",
       "      <td>1.125811</td>\n",
       "      <td>0.749419</td>\n",
       "      <td>0.808617</td>\n",
       "      <td>0.639663</td>\n",
       "      <td>0.567287</td>\n",
       "      <td>1.086538</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.358795</td>\n",
       "      <td>-1.232172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.786533</td>\n",
       "      <td>0.881037</td>\n",
       "      <td>0.993171</td>\n",
       "      <td>0.980998</td>\n",
       "      <td>1.326211</td>\n",
       "      <td>0.937306</td>\n",
       "      <td>0.881132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.148382</td>\n",
       "      <td>-0.157837</td>\n",
       "      <td>0.480372</td>\n",
       "      <td>0.531516</td>\n",
       "      <td>-0.812877</td>\n",
       "      <td>0.656366</td>\n",
       "      <td>-0.504052</td>\n",
       "      <td>-1.600426</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.880755</td>\n",
       "      <td>0.841640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.731527</td>\n",
       "      <td>0.927885</td>\n",
       "      <td>0.994248</td>\n",
       "      <td>0.849990</td>\n",
       "      <td>0.679619</td>\n",
       "      <td>0.708818</td>\n",
       "      <td>0.659484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.596793</td>\n",
       "      <td>-1.779495</td>\n",
       "      <td>-1.126612</td>\n",
       "      <td>1.260549</td>\n",
       "      <td>1.223489</td>\n",
       "      <td>1.086828</td>\n",
       "      <td>-0.538711</td>\n",
       "      <td>0.978633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017029</td>\n",
       "      <td>0.039792</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>0.971491</td>\n",
       "      <td>1.055517</td>\n",
       "      <td>1.026137</td>\n",
       "      <td>0.899098</td>\n",
       "      <td>0.885184</td>\n",
       "      <td>0.955512</td>\n",
       "      <td>0.997989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.342959</td>\n",
       "      <td>-1.560352</td>\n",
       "      <td>-1.035615</td>\n",
       "      <td>1.087132</td>\n",
       "      <td>-1.542449</td>\n",
       "      <td>1.909094</td>\n",
       "      <td>-0.038644</td>\n",
       "      <td>-1.704649</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.854937</td>\n",
       "      <td>0.569733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.530849</td>\n",
       "      <td>0.765882</td>\n",
       "      <td>0.974091</td>\n",
       "      <td>0.813853</td>\n",
       "      <td>1.073043</td>\n",
       "      <td>1.375526</td>\n",
       "      <td>1.112935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.567328</td>\n",
       "      <td>-0.511388</td>\n",
       "      <td>-1.080559</td>\n",
       "      <td>0.669139</td>\n",
       "      <td>1.023909</td>\n",
       "      <td>1.140876</td>\n",
       "      <td>1.259547</td>\n",
       "      <td>-0.007149</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>1.251273</td>\n",
       "      <td>-1.450807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.880216</td>\n",
       "      <td>1.154037</td>\n",
       "      <td>0.981374</td>\n",
       "      <td>0.834201</td>\n",
       "      <td>0.863685</td>\n",
       "      <td>1.218149</td>\n",
       "      <td>1.015226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.571171</td>\n",
       "      <td>-1.237968</td>\n",
       "      <td>0.384381</td>\n",
       "      <td>1.793831</td>\n",
       "      <td>-1.403586</td>\n",
       "      <td>1.019680</td>\n",
       "      <td>-0.642685</td>\n",
       "      <td>0.488566</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.597595</td>\n",
       "      <td>-1.504079</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>0.871877</td>\n",
       "      <td>0.804360</td>\n",
       "      <td>1.401270</td>\n",
       "      <td>1.301220</td>\n",
       "      <td>0.859798</td>\n",
       "      <td>0.897783</td>\n",
       "      <td>1.038412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.934810</td>\n",
       "      <td>1.846591</td>\n",
       "      <td>-0.310408</td>\n",
       "      <td>0.683469</td>\n",
       "      <td>0.301191</td>\n",
       "      <td>0.877597</td>\n",
       "      <td>-1.209096</td>\n",
       "      <td>-1.701322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.885663</td>\n",
       "      <td>0.450982</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>1.266948</td>\n",
       "      <td>1.219701</td>\n",
       "      <td>0.990643</td>\n",
       "      <td>0.739438</td>\n",
       "      <td>0.593967</td>\n",
       "      <td>0.996407</td>\n",
       "      <td>0.859540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50001 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       target  lepton_pT  lepton_eta  lepton_phi  missing_energy_magnitude  \\\n",
       "0         1.0   0.869293   -0.635082    0.225690                  0.327470   \n",
       "1         1.0   0.798835    1.470639   -1.635975                  0.453773   \n",
       "2         0.0   0.367665   -0.881496    0.232349                  1.258956   \n",
       "3         0.0   0.610152   -0.720791    0.047025                  1.125811   \n",
       "4         0.0   1.148382   -0.157837    0.480372                  0.531516   \n",
       "...       ...        ...         ...         ...                       ...   \n",
       "49996     1.0   0.596793   -1.779495   -1.126612                  1.260549   \n",
       "49997     0.0   0.342959   -1.560352   -1.035615                  1.087132   \n",
       "49998     1.0   0.567328   -0.511388   -1.080559                  0.669139   \n",
       "49999     1.0   0.571171   -1.237968    0.384381                  1.793831   \n",
       "50000     0.0   0.934810    1.846591   -0.310408                  0.683469   \n",
       "\n",
       "       missing_energy_phi   jet_1pt  jet_1eta  jet_1phi  jet_1b-tag  ...  \\\n",
       "0               -0.689993  0.754202 -0.248573 -1.092064    0.000000  ...   \n",
       "1                0.425629  1.104875  1.282322  1.381664    0.000000  ...   \n",
       "2                1.728993  0.726262  0.981292 -0.303185    0.000000  ...   \n",
       "3                0.749419  0.808617  0.639663  0.567287    1.086538  ...   \n",
       "4               -0.812877  0.656366 -0.504052 -1.600426    2.173076  ...   \n",
       "...                   ...       ...       ...       ...         ...  ...   \n",
       "49996            1.223489  1.086828 -0.538711  0.978633    0.000000  ...   \n",
       "49997           -1.542449  1.909094 -0.038644 -1.704649    2.173076  ...   \n",
       "49998            1.023909  1.140876  1.259547 -0.007149    2.173076  ...   \n",
       "49999           -1.403586  1.019680 -0.642685  0.488566    2.173076  ...   \n",
       "50000            0.301191  0.877597 -1.209096 -1.701322    0.000000  ...   \n",
       "\n",
       "       jet_4eta  jet_4phi  jet_4b-tag      m_jj     m_jjj      m_lv     m_jlv  \\\n",
       "0     -0.010455 -0.045767    3.101961  1.353760  0.979563  0.978076  0.920005   \n",
       "1      1.128848  0.900461    0.000000  0.909753  1.108330  0.985692  0.951331   \n",
       "2      1.871726 -0.460841    0.000000  0.806875  0.757239  0.987123  0.881511   \n",
       "3     -1.358795 -1.232172    0.000000  0.786533  0.881037  0.993171  0.980998   \n",
       "4     -0.880755  0.841640    0.000000  0.731527  0.927885  0.994248  0.849990   \n",
       "...         ...       ...         ...       ...       ...       ...       ...   \n",
       "49996  0.017029  0.039792    3.101961  0.971491  1.055517  1.026137  0.899098   \n",
       "49997 -0.854937  0.569733    0.000000  0.530849  0.765882  0.974091  0.813853   \n",
       "49998  1.251273 -1.450807    0.000000  0.880216  1.154037  0.981374  0.834201   \n",
       "49999 -0.597595 -1.504079    3.101961  0.871877  0.804360  1.401270  1.301220   \n",
       "50000  0.885663  0.450982    3.101961  1.266948  1.219701  0.990643  0.739438   \n",
       "\n",
       "           m_bb     m_wbb    m_wwbb  \n",
       "0      0.721657  0.988751  0.876678  \n",
       "1      0.803252  0.865924  0.780118  \n",
       "2      0.625056  0.869624  1.198134  \n",
       "3      1.326211  0.937306  0.881132  \n",
       "4      0.679619  0.708818  0.659484  \n",
       "...         ...       ...       ...  \n",
       "49996  0.885184  0.955512  0.997989  \n",
       "49997  1.073043  1.375526  1.112935  \n",
       "49998  0.863685  1.218149  1.015226  \n",
       "49999  0.859798  0.897783  1.038412  \n",
       "50000  0.593967  0.996407  0.859540  \n",
       "\n",
       "[50001 rows x 29 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['target','lepton_pT','lepton_eta','lepton_phi','missing_energy_magnitude','missing_energy_phi',\n",
    "'jet_1pt','jet_1eta','jet_1phi','jet_1b-tag','jet_2pt','jet_2eta','jet_2phi','jet_2b-tag',\n",
    "'jet_3pt','jet_3eta','jet_3phi','jet_3b-tag','jet_4pt','jet_4eta','jet_4phi','jet_4b-tag',\n",
    "'m_jj','m_jjj','m_lv','m_jlv','m_bb','m_wbb','m_wwbb']\n",
    "higgs_ds.rename(columns=dict(zip(higgs_ds.columns, features)),inplace=True)\n",
    "higgs_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "orange-anderson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50001 entries, 0 to 50000\n",
      "Data columns (total 29 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   target                    50001 non-null  float64\n",
      " 1   lepton_pT                 50001 non-null  float64\n",
      " 2   lepton_eta                50001 non-null  float64\n",
      " 3   lepton_phi                50001 non-null  float64\n",
      " 4   missing_energy_magnitude  50001 non-null  float64\n",
      " 5   missing_energy_phi        50001 non-null  float64\n",
      " 6   jet_1pt                   50001 non-null  float64\n",
      " 7   jet_1eta                  50001 non-null  float64\n",
      " 8   jet_1phi                  50001 non-null  float64\n",
      " 9   jet_1b-tag                50001 non-null  float64\n",
      " 10  jet_2pt                   50001 non-null  float64\n",
      " 11  jet_2eta                  50001 non-null  float64\n",
      " 12  jet_2phi                  50001 non-null  float64\n",
      " 13  jet_2b-tag                50001 non-null  float64\n",
      " 14  jet_3pt                   50001 non-null  float64\n",
      " 15  jet_3eta                  50001 non-null  float64\n",
      " 16  jet_3phi                  50001 non-null  float64\n",
      " 17  jet_3b-tag                50001 non-null  float64\n",
      " 18  jet_4pt                   50001 non-null  float64\n",
      " 19  jet_4eta                  50001 non-null  float64\n",
      " 20  jet_4phi                  50001 non-null  float64\n",
      " 21  jet_4b-tag                50001 non-null  float64\n",
      " 22  m_jj                      50001 non-null  float64\n",
      " 23  m_jjj                     50001 non-null  float64\n",
      " 24  m_lv                      50001 non-null  float64\n",
      " 25  m_jlv                     50001 non-null  float64\n",
      " 26  m_bb                      50001 non-null  float64\n",
      " 27  m_wbb                     50001 non-null  float64\n",
      " 28  m_wwbb                    50001 non-null  float64\n",
      "dtypes: float64(29)\n",
      "memory usage: 11.1 MB\n"
     ]
    }
   ],
   "source": [
    "higgs_ds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-cloud",
   "metadata": {},
   "source": [
    "## More on Data Quality\n",
    "\n",
    "The HIGGS data set is nearly balanced, with 52.99% positive examples, that’s why we did not perform subsampling or oversampling on the data.\n",
    "\n",
    "|| label |count|%|\n",
    "| --- | --- |--- | --- |\n",
    "|signal| 1 |5829123| 52.99%|\n",
    "|background| 0 | 5170877 | 47.01%|\n",
    "\n",
    "The paper mentioned that various numbers of examples were used for different stages of their study.\n",
    " * Hyper-parameters selection: Using a subset of the HIGGS data consisting of 2.6 million training examples and 100,000 validation examples.\n",
    " * Hyper-parameters optimization: Using complete 11 million examples. \n",
    " * Performance testing: Classifiers were tested on 500,000 simulated examples generated from the same Monte Carlo procedures as the training sets.\n",
    " \n",
    "As the paper was published in 2014, the original study might be not thorough due to expensive computational cost. In this case study, the full dataset (11 million data) is used to build and validate models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-pavilion",
   "metadata": {},
   "source": [
    "## Sampled Data \n",
    "\n",
    "Randomly sample data too has sample proportion of singal (52.96%) and background (47.04%) observations which is nearly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "authentic-destiny",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>23490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>26511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  counts\n",
       "0     0.0   23490\n",
       "1     1.0   26511"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "higgs_ds.groupby(['target']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-truck",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "\n",
    "Prior to strated building the model, we studied the original arictecture stated in the paper. In this case study, we use TensorFlow to architect a Neural Network to replicate the results presented in [2]. We first load the HIGGS Data [1], explore data analysis and then split the data into train and test data sets (80/20). \n",
    "\n",
    "Then we build the NN models and then attempt to optimize the hyperparameters like the number of layers, the number of neurons and the learning rate. We assigned the same initial learning rate used in the original paper and used the exponential decay learning rate method during the training of the model. We used AUC as a metric to measure the goodness of the NN models to perform an apple to apple comparison to the model found in [2]. Finally, we visualized AUC trends for both train and test data\n",
    "and the learning rate decay curve through the model training process.\n",
    "\n",
    "\n",
    "We used TensorFlow with Keras package to build our model. To replicate the original\n",
    "model in the paper, we used the same hyperparameters from the paper as listed below.\n",
    "* Neurons = 300 units per layer\n",
    "* Number of Layers = 5 layers (4 Dense layers + 1 output layer)\n",
    "* Activation Function = tanh for the Dense layers, sigmoid for output layer\n",
    "* Initial Learning Rate = 0.05\n",
    "* Learning Rate Decay = Exponential Decay\n",
    "* Momentum = 0.9\n",
    "* Metrics = AUC\n",
    "* Epoch's = 20 (for all models) and 100 (for two first models)\n",
    "* Batch Size = 100\n",
    "\n",
    "......"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-mountain",
   "metadata": {},
   "source": [
    "# Recommendation & Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-garage",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-village",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-naples",
   "metadata": {},
   "source": [
    "1. [HIGGS DataSet - UCI- Website](https://archive.ics.uci.edu/ml/datasets/HIGGS)\n",
    "2. P. Onyisi, Higgs boson FAQ, University of Texas ATLAS group, 2013.\n",
    "3. Baldi, P., Sadowski, P. & Whiteson, D. Searching for exotic particles in high-energy physics with deep learning. Nat Commun 5, 4308 (2014). https://doi.org/10.1038/ncomms5308\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-instruction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
